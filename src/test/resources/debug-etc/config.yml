#
# dd-dataverse-ingest configuration file
#

#
# See https://www.dropwizard.io/en/latest/manual/configuration.html#server
#
server:
  applicationContextPath: /
  adminContextPath: /
  applicationConnectors:
    - type: http
      port: 20360
  adminConnectors:
    - type: http
      port: 20361

#
# Parameters related to communication with the Dataverse instance
#
dataverse:
  baseUrl: 'http://dev.archaeology.datastations.nl:8080'
  apiKey: 'changeme'
  unblockKey: 's3kretKey'
  awaitLockStateMaxNumberOfRetries: 30
  awaitLockStateMillisecondsBetweenRetries: 500
  httpClient:
    timeout: 30s
    connectionTimeout: 15s
    connectionRequestTimeout: 15s
    timeToLive: 1h
    retries: 2
    userAgent: dd-dataverse-ingest
#
# Settings related to ingest services.
#
ingest:
  # Deposits dropped in the inbox will automatically be picked up and processed by the ingest service.
  autoIngest:
    inbox: data/auto-ingest/inbox
    outbox: data/auto-ingest/outbox
  # Import of migration deposits. This area is used to migrate datasets from EASY to Dataverse and differs from import only in the way the DANS deposits are converted
  # to Dataverse Ingest deposits.
  migration:
    inbox: data/migration/deposits
    outbox: data/migration/out
  # Import of deposits. This area is used for manual bulk imports of deposits.
  import:
    inbox: data/import/inbox
    outbox: data/import/outbox
  tempDir: data/tmp
  #
  # If some of the metadata blocks are secured with a secret key, the key must be included in the metadataKeys map below, as follows:
  #
  # metadataKeys:
  #    <metadata-block-name>: '<secret-key-value>'
  #
  metadataKeys:
    dansDataVaultMetadata: password_vault_metadata
  #
  # The maximum number of files that the service will upload in a single batch. If :ZipUploadFilesLimit is set to a lower number than this,
  # the lower number will be used, so that Dataverse will not include the batch as one zip file in the dataset instead of unpacking it.
  #
  # See: https://guides.dataverse.org/en/latest/installation/config.html#zipuploadfileslimit
  #
  maxNumberOfFilesPerUploadBatch: 1000

  #
  # The service waits for the dataset to reach the released state before it continues processing the next deposit. These settings control how long the service waits,
  # before giving up and marking the deposit as failed.
  #
  waitForReleasedState:
    # Start polling for the dataset state after this time x number of files in the dataset. It is expected that releasing a dataset takes at least this amount of time.
    leadTimePerFile: 200ms
    # Give up waiting for the dataset to be in the expected state after this time. The lead time is *not* included in this timeout.
    timeout: 5m
    # The interval between polling the dataset state.
    pollingInterval: 1s

#
# Settings related to the conversion of deposits from the legacy format to the format used by the ingest service. Set to null to disable.
#
dansDepositConversion:
  #
  # Filtering. Files with a path matching the pattern will not be added to the dataset. Renaming/moving files is not affected.
  # By default, no files are excluded.
  #
  fileExclusionPattern: 'a^'

  #
  # Files that must be uploaded individually, and not in batch. By default, no files are uploaded individually.
  #
  filesForSeparateUploadPattern: 'a^'

  #
  # Map from depositor.userId to organization name. Used to fill in the dansDataSupplier metadata field. If there is no
  # entry for a depositor, dansDataSupplier will be left empty.
  #
  dataSuppliers:
  # user001: The Organization Name

  #
  # If true, try to deduplicate multi-value metadata fields. If false, do not deduplicate.
  #
  deduplicate: false

  #
  # The directory where various metadata term mappings are stored.
  #
  mappingDefsDir: etc/

  #
  # Role to assign to the depositor of the dataset, per use-case.
  #
  assignDepositorRole:
    autoIngest: swordupdater
    migration: contributorplus

  #
  # Roles the depositor must have to be able to edit and publish the dataset, per use-case.
  #
  depositorAuthorization:
    autoIngest:
      editDataset: swordupdater
      publishDataset: swordpublisher
    migration:
      editDataset: contributorplus
      publishDataset: dsContributor

  validateDansBag:
    url: 'http://localhost:20330/'
    healthCheck:
      name: dd-validate-dans-bag
      pingUrl: 'http://localhost:20331/ping'
    httpClient:
      timeout: 45min
      connectionTimeout: 15s
      connectionRequestTimeout: 15s
      timeToLive: 1h
      retries: 2
      userAgent: dd-dataverse-ingest

#
# Health check scheduling
#
health:
  delayedShutdownHandlerEnabled: false
  initialOverallState: false
  healthChecks:
    - name: dataverse
      critical: true
      initialState: false
      schedule:
        checkInterval: 5s
    - name: dd-validate-dans-bag
      critical: true
      initialState: false
      schedule:
        checkInterval: 5s

dependenciesReadyCheck:
  healthChecks:
    - dataverse
    - dd-validate-dans-bag
  pollInterval: 5s


#
# See https://www.dropwizard.io/en/latest/manual/configuration.html#logging
#
logging:
  level: INFO
  appenders:
    - type: console
      logFormat: "%-5p [%d{ISO8601}] [%t] %c: %m%n%rEx"

    - type: file
      archive: false
      timeZone: system
      currentLogFilename: data/dd-dataverse-ingest.log
  loggers:
    'nl.knaw.dans.dvingest': 'DEBUG'
    'org.hibernate.engine.internal.StatisticalLoggingSessionEventListener': 'OFF'
